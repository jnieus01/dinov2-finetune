{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune DINOv2 for Domain Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchrl.trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from model import DINOLinearClassifier\n",
    "from load_data import *\n",
    "from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "from transformers import AutoImageProcessor, AutoModel, TrainingArguments\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from dataclasses import dataclass\n",
    "from PIL import Image\n",
    "import requests\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    # Place all input tensors on the same device as the model\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = DINOLinearClassifier(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device),\n",
    "                             reduction=\"none\")\n",
    "    # Place outputs on CPU for compatibility with other dataset columns\n",
    "    return {\"loss\": loss.cpu().numpy(),\n",
    "            \"predicted_label\": pred_label.cpu().numpy()}\n",
    "\n",
    "\n",
    "def time_pipeline(self, query):\n",
    "    latencies = []\n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        _ = self.pipeline(query)\n",
    "    # Timed run\n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        _ = self.pipeline(query)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
    "    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a479113c24cb43c898f0ec42be3ea5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/436 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DINOLinearClassifier(\n",
       "  (model): DinoVisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-23): 24 x NestedTensorBlock(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MemEffAttention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DINOLinearClassifier(num_classes=2)\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "# model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DINOLinearClassifier' object has no attribute 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/workspace/geopacha/jn/finetune.py:118\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m# training_args = TrainingArguments(output_dir=\"./output\",\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m#                                 #output_dir=model_name,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m#                                 num_train_epochs=2,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m#                 train_dataset=dataset[\"train\"],\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m#                 eval_dataset=dataset[\"validation\"])\u001b[39;00m\n\u001b[1;32m    116\u001b[0m model\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    117\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam([\n\u001b[0;32m--> 118\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: model\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39mpatch_embed\u001b[39m.\u001b[39mparameters()},\n\u001b[1;32m    119\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: model\u001b[39m.\u001b[39mclassifier\u001b[39m.\u001b[39mparameters()}\n\u001b[1;32m    120\u001b[0m ], lr\u001b[39m=\u001b[39m\u001b[39m5e-5\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining...\u001b[39m\u001b[39m\"\u001b[39m)                \n\u001b[1;32m    123\u001b[0m \u001b[39m# Training and validation loop\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DINOLinearClassifier' object has no attribute 'transformer'"
     ]
    }
   ],
   "source": [
    "%run finetune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "in_channel = model.fc.in_features\n",
    "model.fc = nn.Linear(in_channel, 2) # 2 classes\n",
    "\n",
    "# Set the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define transformations to the data inputs\n",
    "data_transforms = {\n",
    "    \"train\": \n",
    "        torchvision.transforms.Compose([\n",
    "            transforms.Resize(size=(196,196), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    \"validation\": \n",
    "        torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((196, 196)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "}\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_dataset = GeoPACHAImageDataset(data_file=\"/workspace/geopacha/jn/Train.csv\",\n",
    "                            transform=data_transforms[\"train\"])\n",
    "validation_dataset = GeoPACHAImageDataset(data_file=\"/workspace/geopacha/jn/Train.csv\",\n",
    "                            transform=data_transforms[\"validation\"])\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                            num_workers=4),\n",
    "    \"validation\": torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                            num_workers=4)\n",
    "}\n",
    "\n",
    "trainiter = iter(dataloaders[\"train\"])\n",
    "validationiter = iter(dataloaders[\"validation\"])\n",
    "\n",
    "logging_steps = len(train_dataset) // batch_size \n",
    "model_name = f\"dinov2_vitl14-finetuned\" \n",
    "\n",
    "config = TrainingConfig(epochs=2, batch_size=batch_size, learning_rate=2e-5)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./output\",\n",
    "                                #output_dir=model_name,\n",
    "                                num_train_epochs=2,\n",
    "                                learning_rate=2e-5,\n",
    "                                per_device_train_batch_size=batch_size,\n",
    "                                per_device_eval_batch_size=batch_size,\n",
    "                                weight_decay=0.01,\n",
    "                                evaluation_strategy=\"epoch\",\n",
    "                                disable_tqdm=False,\n",
    "                                logging_steps=logging_steps,\n",
    "                                push_to_hub=True,\n",
    "                                log_level=\"error\")\n",
    "\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "                compute_metrics=compute_metrics,\n",
    "                train_dataset=dataset[\"train\"],\n",
    "                eval_dataset=dataset[\"validation\"])\n",
    "\n",
    "model.to(config.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "print(\"Training...\")                \n",
    "# Training and validation loop\n",
    "for epoch in range(config.epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in dataloaders[\"train\"]:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(config.device), labels.to(config.device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_accuracy = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloaders[\"validation\"]:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(config.device), labels.to(config.device)\n",
    "            outputs = model(inputs)\n",
    "            loss = cross_entropy(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = outputs.argmax(dim=1, keepdim=True)\n",
    "            correct = predictions.eq(labels.view_as(predictions)).sum().item()\n",
    "            val_accuracy.append(correct / len(labels))\n",
    "\n",
    "    # Calculate average losses and accuracy\n",
    "    avg_train_loss = train_loss / len(dataloaders[\"train\"])\n",
    "    avg_val_loss = val_loss / len(dataloaders[\"validation\"])\n",
    "    avg_val_accuracy = sum(val_accuracy) / len(val_accuracy)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{config.epochs}, Train Loss: {avg_train_loss:.4f}, '\n",
    "        f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "preds_output = trainer.predict(dataset[\"validation\"])\n",
    "preds_output.metrics\n",
    "y_preds = np.argmax(preds_out_size\n",
    "put.predictions, axis=1)\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)\n",
    "\n",
    "# Convert dataset back to PyTorch tensors\n",
    "dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# Compute loss values\n",
    "dataset[\"validation\"] = dataset[\"validation\"].map(\n",
    "    forward_pass_with_label, batched=True, batch_size=16)\n",
    "\n",
    "dataset.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "df_test = dataset[\"validation\"][:][cols]\n",
    "\n",
    "def label_int2str(row):\n",
    "    return df_test[\"train\"].features[\"label\"].int2str(row)\n",
    "\n",
    "df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n",
    "df_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n",
    "                            .apply(label_int2str))\n",
    "df_test.sort_values(\"loss\", ascending=True).head(10)\n",
    "\n",
    "save_model_path = \"dinov2_vitl14-finetuned.pth\"\n",
    "torch.save(model.state_dict(), save_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model.py\n",
    "%run model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "train_config = TrainingConfig()\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": \n",
    "        torchvision.transforms.Compose([\n",
    "            transforms.Resize(size=(196,196), interpolation=transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    \"validation\": \n",
    "        torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(size=(196, 196), antialias=True),\n",
    "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "}\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_dataset = GeoPACHAImageDataset(data_file=\"/workspace/geopacha/jn/Train.csv\",\n",
    "                            transform=data_transforms[\"train\"])\n",
    "validation_dataset = GeoPACHAImageDataset(data_file=\"/workspace/geopacha/jn/Train.csv\",\n",
    "                            transform=data_transforms[\"validation\"])\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(train_dataset, batch_size=train_config.batch_size, shuffle=True, num_workers=4),\n",
    "    \"validation\": torch.utils.data.DataLoader(validation_dataset, batch_size=train_config.batch_size, shuffle=True, num_workers=4)\n",
    "}\n",
    "print(\"Data loaded successfully!\")\n",
    "dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "model = DINOLinearClassifier(model=dinov2_vitl14, num_classes=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# freeze and unfreeze specific layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.transformer.patch_embed.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "in_channel = model.fc.in_features\n",
    "model.fc = nn.Linear(in_channel, 2)\n",
    "# model = initialize_model(train_config)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=train_config.learning_rate)\n",
    "train_config.logging_steps = len(train_dataset) // train_config.batch_size\n",
    "print(\"Training model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DINOLinearClassifier(\n",
       "  (transformer): DinoVisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-23): 24 x NestedTensorBlock(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MemEffAttention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloaders[\"train\"]:\n",
    "        inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7201, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1024])\n"
     ]
    }
   ],
   "source": [
    "x = model.transformer(inputs)\n",
    "print(x.shape)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3060,  0.1102,  0.7062,  ...,  1.2252, -0.3764,  0.3686],\n",
      "        [-2.8179,  0.4306, -1.0216,  ...,  0.0432, -1.0885,  1.1637],\n",
      "        [-1.9625,  0.9733, -0.0866,  ...,  0.4928, -0.2759,  1.3728],\n",
      "        ...,\n",
      "        [-0.7031,  1.4322,  0.8492,  ..., -1.1186, -0.9884, -1.0179],\n",
      "        [-0.0903, -1.0424,  0.0894,  ...,  1.3567,  0.2269,  0.7433],\n",
      "        [-1.9105,  0.7407,  1.5410,  ...,  0.6182,  0.1354, -0.3090]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1024])\n",
      "tensor([[-1.9767e+00, -2.6672e-03,  1.3012e+00,  ...,  1.0498e+00,\n",
      "         -5.0951e-01,  8.2622e-01],\n",
      "        [-2.3909e+00,  3.0167e-01, -4.4963e-01,  ..., -3.3164e-01,\n",
      "         -1.2156e+00,  2.3932e+00],\n",
      "        [-1.5876e+00,  8.3849e-01,  5.0144e-01,  ...,  1.9505e-01,\n",
      "         -3.8440e-01,  2.8392e+00],\n",
      "        ...,\n",
      "        [-4.2304e-01,  1.3032e+00,  1.4562e+00,  ..., -1.6297e+00,\n",
      "         -1.0962e+00, -1.9340e+00],\n",
      "        [ 1.2658e-01, -1.1701e+00,  6.7833e-01,  ...,  1.2651e+00,\n",
      "          1.2955e-01,  1.6642e+00],\n",
      "        [-1.6253e+00,  6.3244e-01,  2.1818e+00,  ...,  3.5243e-01,\n",
      "          2.1357e-02, -5.7369e-01]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = model.transformer.norm(x)\n",
    "print(x.shape) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2])\n",
      "tensor([[ 0.2109, -0.8287],\n",
      "        [ 0.4512, -0.9362],\n",
      "        [-0.2077, -0.6444],\n",
      "        [ 0.1354, -0.7096],\n",
      "        [ 0.3738, -1.1583],\n",
      "        [-0.0447, -0.6878],\n",
      "        [ 0.4727, -0.9389],\n",
      "        [ 0.1762, -1.4110],\n",
      "        [ 0.2367,  0.0023],\n",
      "        [-0.2615, -0.8846],\n",
      "        [ 0.1196, -0.4694],\n",
      "        [ 0.1132, -0.7432],\n",
      "        [ 0.2245, -0.9475],\n",
      "        [ 0.0714, -0.7980],\n",
      "        [ 0.3627, -0.8589],\n",
      "        [-0.4030, -0.5342],\n",
      "        [-0.0837, -0.6425],\n",
      "        [ 0.0406, -0.8466],\n",
      "        [ 0.2901, -1.1164],\n",
      "        [ 0.5875, -0.4292],\n",
      "        [ 0.4975, -1.3442],\n",
      "        [ 0.0809, -0.4946],\n",
      "        [-0.0276, -0.3209],\n",
      "        [ 0.4913, -0.0383],\n",
      "        [ 0.0156, -0.7508],\n",
      "        [ 0.1262, -0.2842],\n",
      "        [ 0.2187, -0.4185],\n",
      "        [ 0.6525, -0.9659],\n",
      "        [ 0.2354, -0.3593],\n",
      "        [ 0.4040, -0.2739],\n",
      "        [ 0.3303, -0.8400],\n",
      "        [ 0.4357, -0.8584],\n",
      "        [ 0.0700, -0.6281],\n",
      "        [ 0.2209, -0.9860],\n",
      "        [ 0.2395, -0.6596],\n",
      "        [ 0.6295, -0.7446],\n",
      "        [ 0.4681, -0.4593],\n",
      "        [ 0.1328, -0.6718],\n",
      "        [ 0.6308, -1.0282],\n",
      "        [-0.0278, -1.0312],\n",
      "        [ 0.2522, -0.7912],\n",
      "        [ 0.4931, -0.8665],\n",
      "        [ 0.2186, -0.5115],\n",
      "        [ 0.5053, -0.7971],\n",
      "        [ 0.6937, -0.6442],\n",
      "        [-0.0087, -0.5982],\n",
      "        [ 0.0852, -1.0311],\n",
      "        [ 0.3092, -0.5979],\n",
      "        [ 0.4165, -0.8780],\n",
      "        [ 0.2988, -0.3485],\n",
      "        [-0.0366, -0.7270],\n",
      "        [ 0.1765, -0.5553],\n",
      "        [-0.1056, -0.5993],\n",
      "        [ 0.3363, -0.5339],\n",
      "        [ 0.4430, -0.4153],\n",
      "        [ 0.6450, -0.7093],\n",
      "        [ 0.0918, -0.1622],\n",
      "        [ 0.3065, -0.5114],\n",
      "        [ 0.1184, -0.2753],\n",
      "        [ 0.1885, -0.8153],\n",
      "        [-0.0342, -1.0775],\n",
      "        [ 0.5158, -0.5460],\n",
      "        [ 0.8852, -0.2777],\n",
      "        [-0.0760, -0.7982]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = model.classifier(x)\n",
    "print(x.shape) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/workspace/geopacha/miniconda3/envs/sarl_env_xformers/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[ 6.4910e-01, -7.6930e-01],\n",
      "        [ 1.3468e-04, -1.2950e-01],\n",
      "        [ 1.7471e-01, -1.1779e+00],\n",
      "        [-9.8383e-02, -8.3284e-01],\n",
      "        [ 1.9254e-01, -3.9445e-01],\n",
      "        [ 4.3091e-01, -6.5082e-01],\n",
      "        [ 1.8002e-01, -4.0405e-01],\n",
      "        [ 6.4127e-02, -4.8206e-01],\n",
      "        [ 4.0940e-01, -4.1448e-01],\n",
      "        [ 2.5954e-01, -7.3724e-01],\n",
      "        [ 2.5878e-01, -4.3306e-01],\n",
      "        [ 3.8847e-01, -8.9892e-01],\n",
      "        [ 8.8519e-01, -2.7769e-01],\n",
      "        [ 2.1577e-01, -5.4025e-01],\n",
      "        [ 4.2978e-01, -1.2385e+00],\n",
      "        [ 5.1343e-01, -3.2131e-01],\n",
      "        [ 6.3679e-01, -1.0904e+00],\n",
      "        [ 2.3013e-01, -7.1488e-01],\n",
      "        [ 3.3204e-01, -8.4470e-01],\n",
      "        [ 1.0612e-01, -5.6339e-01],\n",
      "        [ 4.4704e-01, -1.4843e-02],\n",
      "        [ 3.4076e-01,  1.5775e-02],\n",
      "        [ 3.9529e-01, -6.8596e-01],\n",
      "        [-1.0023e-01, -4.2356e-02],\n",
      "        [ 4.1647e-01, -8.7797e-01],\n",
      "        [ 1.2776e-01, -4.3821e-01],\n",
      "        [ 5.2293e-01, -1.2496e+00],\n",
      "        [ 3.9765e-01, -9.8345e-01],\n",
      "        [ 2.9598e-01, -9.4520e-01],\n",
      "        [ 4.2929e-01, -6.1642e-01],\n",
      "        [ 6.7456e-01, -1.4683e-01],\n",
      "        [ 2.1947e-01, -5.5686e-01],\n",
      "        [ 5.9796e-01, -5.9223e-01],\n",
      "        [ 1.2124e-01, -1.1203e+00],\n",
      "        [ 1.2823e-01, -8.7605e-01],\n",
      "        [ 2.5233e-01, -3.3584e-01],\n",
      "        [ 2.6216e-01, -7.3960e-01],\n",
      "        [-7.9051e-02, -1.0141e+00],\n",
      "        [ 2.5345e-01, -6.2797e-01],\n",
      "        [ 2.7246e-01, -2.3793e-01],\n",
      "        [ 2.3952e-01, -6.5959e-01],\n",
      "        [-1.3466e-01, -1.5321e-01],\n",
      "        [ 4.8678e-01, -9.5263e-01],\n",
      "        [ 3.3420e-02, -4.1476e-01],\n",
      "        [ 3.4124e-03, -8.6980e-01],\n",
      "        [-1.2622e-01, -5.5211e-02],\n",
      "        [ 1.6448e-01, -9.0618e-01],\n",
      "        [ 3.3660e-02, -1.1898e+00],\n",
      "        [ 4.6892e-01, -8.9190e-01],\n",
      "        [ 4.4824e-01, -2.8623e-01],\n",
      "        [ 2.1289e-01, -9.1591e-01],\n",
      "        [ 2.4975e-01, -3.7321e-01],\n",
      "        [ 1.6805e-01, -4.6278e-01],\n",
      "        [ 1.0278e-01, -2.4302e-01],\n",
      "        [ 1.6801e-01, -5.3410e-01],\n",
      "        [ 2.1682e-02, -7.1074e-01],\n",
      "        [ 3.9973e-01, -8.5260e-01],\n",
      "        [ 6.6313e-01, -1.1496e+00],\n",
      "        [ 4.1463e-01, -1.3985e+00],\n",
      "        [ 3.8536e-01, -4.7334e-01],\n",
      "        [ 3.3665e-01, -3.4718e-01],\n",
      "        [ 8.2001e-02, -7.7964e-01],\n",
      "        [-2.4401e-01, -5.7737e-01],\n",
      "        [ 6.9838e-01, -9.3121e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[ 0.1295, -0.9548],\n",
      "        [-0.0905, -0.3869],\n",
      "        [ 0.0871, -0.7580],\n",
      "        [ 0.1946, -0.7766],\n",
      "        [ 0.3802, -0.5453],\n",
      "        [ 0.4538, -0.9323],\n",
      "        [ 0.2582, -0.1975],\n",
      "        [ 0.2531, -0.4300],\n",
      "        [-0.1597, -0.6611],\n",
      "        [ 0.1212, -0.8516],\n",
      "        [ 0.8045, -0.6688],\n",
      "        [-0.1037, -0.6252],\n",
      "        [ 0.0646, -0.8615],\n",
      "        [ 0.0136, -0.7153],\n",
      "        [ 0.6430, -0.8688],\n",
      "        [-0.0147, -0.6170],\n",
      "        [ 0.0334, -0.2269],\n",
      "        [ 0.0448, -1.0462],\n",
      "        [ 0.5172, -0.5019],\n",
      "        [ 0.4383, -0.7098],\n",
      "        [ 0.0341, -0.7749],\n",
      "        [ 0.3874, -0.9093],\n",
      "        [-0.0229, -0.7276],\n",
      "        [ 0.2572, -1.1236],\n",
      "        [ 0.3614, -0.4068],\n",
      "        [ 0.0569, -0.2248],\n",
      "        [-0.0521, -0.6633],\n",
      "        [ 0.0069, -0.9346],\n",
      "        [ 0.2579, -0.4042],\n",
      "        [-0.1211, -0.4867],\n",
      "        [ 0.1071, -0.9512],\n",
      "        [-0.1068, -1.2207],\n",
      "        [-0.0022, -0.6697],\n",
      "        [ 0.0614, -0.6595],\n",
      "        [ 0.7598, -0.4051],\n",
      "        [ 0.2159, -0.5070],\n",
      "        [ 0.5447, -0.3171],\n",
      "        [ 0.1090, -0.3885],\n",
      "        [ 0.1313, -0.8126],\n",
      "        [-0.1735, -0.9780],\n",
      "        [ 0.3276, -0.2318],\n",
      "        [ 0.2823, -0.2265],\n",
      "        [ 0.3219, -0.9250],\n",
      "        [ 0.3573, -0.3990],\n",
      "        [ 0.4685, -0.4703],\n",
      "        [ 0.2967, -0.1629],\n",
      "        [-0.0471, -0.5821],\n",
      "        [ 0.4140, -0.8133],\n",
      "        [ 0.2392, -0.5320],\n",
      "        [ 0.1318, -0.2176],\n",
      "        [-0.1941, -0.3582],\n",
      "        [ 0.0769, -0.9111],\n",
      "        [ 0.4007, -1.0306],\n",
      "        [ 0.5478, -0.6585],\n",
      "        [ 0.1209, -0.2788],\n",
      "        [ 0.3541, -1.1387],\n",
      "        [ 0.2586, -0.7451],\n",
      "        [ 0.2699, -0.6844],\n",
      "        [ 0.2312, -0.8841],\n",
      "        [ 0.2802, -0.6722],\n",
      "        [ 0.2882, -0.7652],\n",
      "        [ 0.1039, -0.5301],\n",
      "        [ 0.1542, -0.8682],\n",
      "        [ 0.5276, -0.7257]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[ 0.1099, -1.0513],\n",
      "        [ 0.1194, -0.8220],\n",
      "        [ 0.3782, -0.5357],\n",
      "        [ 0.0022, -0.8261],\n",
      "        [ 0.1496, -0.8293],\n",
      "        [-0.1908, -0.9156],\n",
      "        [ 0.0117, -0.8666],\n",
      "        [ 0.4224, -1.0167],\n",
      "        [ 0.1039, -0.0872],\n",
      "        [ 0.1593, -0.8981],\n",
      "        [-0.1409, -1.0008],\n",
      "        [ 0.0966, -0.8884],\n",
      "        [ 0.5018, -0.8272],\n",
      "        [-0.0418, -0.5679],\n",
      "        [ 0.6455, -0.7123],\n",
      "        [ 0.5392, -0.8473],\n",
      "        [ 0.0037, -0.6034],\n",
      "        [-0.0454, -0.3001],\n",
      "        [ 0.4260, -0.3393],\n",
      "        [ 0.2244, -1.0314],\n",
      "        [-0.1430, -0.7142],\n",
      "        [ 0.3108, -0.4275],\n",
      "        [ 0.2639, -0.8775],\n",
      "        [ 0.0943, -0.8196],\n",
      "        [ 0.4299, -1.1483],\n",
      "        [ 0.4146, -0.6124],\n",
      "        [ 0.1715, -0.3243],\n",
      "        [-0.2051,  0.0308],\n",
      "        [ 0.2173, -0.6261],\n",
      "        [ 0.3927, -1.0017],\n",
      "        [ 0.1659, -0.2644],\n",
      "        [ 0.0128, -1.1625],\n",
      "        [ 0.2681, -1.0704],\n",
      "        [ 0.5362, -0.3733],\n",
      "        [ 0.7025, -0.0973],\n",
      "        [ 0.5013, -0.8134],\n",
      "        [ 0.3284, -0.5345],\n",
      "        [ 0.0379, -0.9885],\n",
      "        [ 0.0352, -0.2758],\n",
      "        [-0.2828, -1.0223],\n",
      "        [ 0.5188, -0.2352],\n",
      "        [-0.2710, -0.6357],\n",
      "        [ 0.1633, -0.4182],\n",
      "        [ 0.0677, -0.6345],\n",
      "        [ 0.1561, -0.7011],\n",
      "        [ 0.5726, -1.1623],\n",
      "        [ 0.3719, -0.3906],\n",
      "        [ 0.1597, -0.0790],\n",
      "        [-0.0339, -0.6500],\n",
      "        [ 0.0095, -0.2554],\n",
      "        [-0.1204, -0.8327],\n",
      "        [ 0.2317, -0.9554],\n",
      "        [ 0.2148, -0.7888],\n",
      "        [ 0.4577, -1.1969],\n",
      "        [ 0.0876, -0.8584],\n",
      "        [ 0.8862,  0.0062],\n",
      "        [ 0.1129, -0.7041],\n",
      "        [-0.1012, -0.1784],\n",
      "        [-0.0022, -0.6038],\n",
      "        [ 0.2744, -0.8498],\n",
      "        [ 0.3795, -0.8353],\n",
      "        [ 0.5572, -0.9388],\n",
      "        [ 0.2630, -0.5340],\n",
      "        [ 0.4208, -1.1186]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-5.3341e-02, -6.7134e-01],\n",
      "        [ 1.1303e-01, -7.4600e-01],\n",
      "        [-4.4534e-02, -8.4245e-01],\n",
      "        [-9.1977e-03, -4.5250e-01],\n",
      "        [ 4.1889e-01, -4.7411e-01],\n",
      "        [-2.6981e-01, -1.1188e+00],\n",
      "        [ 2.1621e-01, -4.0631e-01],\n",
      "        [ 1.3159e-01,  2.4781e-01],\n",
      "        [-2.6995e-01, -5.3096e-01],\n",
      "        [ 1.5216e-01, -7.1652e-01],\n",
      "        [ 2.0369e-01, -4.1825e-01],\n",
      "        [ 1.2185e-01, -7.0474e-01],\n",
      "        [ 3.2593e-01, -2.3218e-01],\n",
      "        [-1.7318e-01, -6.8765e-01],\n",
      "        [ 9.7177e-02, -2.1551e-01],\n",
      "        [ 5.2710e-01, -5.4569e-01],\n",
      "        [ 2.5141e-01, -3.6456e-01],\n",
      "        [ 4.2070e-01, -6.2010e-01],\n",
      "        [ 7.0391e-02, -5.6646e-01],\n",
      "        [ 1.8193e-01, -4.4317e-01],\n",
      "        [-3.2224e-02, -6.5891e-01],\n",
      "        [ 4.7186e-01, -7.1176e-01],\n",
      "        [-3.7135e-01, -1.1411e+00],\n",
      "        [ 1.6128e-01, -3.7157e-01],\n",
      "        [-5.0785e-01, -7.2543e-01],\n",
      "        [ 5.0535e-01, -4.4691e-01],\n",
      "        [ 3.1553e-01, -4.7338e-01],\n",
      "        [-2.0096e-01, -8.2836e-01],\n",
      "        [-8.9576e-02, -2.8608e-01],\n",
      "        [-1.9513e-01, -5.3894e-01],\n",
      "        [-3.5358e-02, -9.6594e-02],\n",
      "        [-1.5112e-01, -2.1950e-01],\n",
      "        [ 4.5020e-01, -9.4791e-01],\n",
      "        [ 2.6856e-01, -6.1905e-01],\n",
      "        [ 2.5530e-01, -1.1221e+00],\n",
      "        [ 3.8039e-01, -6.9282e-01],\n",
      "        [ 1.6236e-01, -7.5371e-01],\n",
      "        [ 1.8750e-01, -4.4446e-01],\n",
      "        [ 5.3934e-01, -7.4796e-01],\n",
      "        [ 1.2431e-01, -8.6397e-01],\n",
      "        [-1.4041e-01, -3.7752e-01],\n",
      "        [ 5.8708e-01, -5.5245e-01],\n",
      "        [-7.6941e-02, -9.6303e-01],\n",
      "        [ 2.2035e-01, -5.1489e-01],\n",
      "        [ 5.2534e-01, -5.2680e-01],\n",
      "        [ 2.6906e-01, -9.4602e-01],\n",
      "        [ 1.5719e-01, -7.1579e-01],\n",
      "        [ 7.5710e-02, -1.0559e+00],\n",
      "        [ 1.6638e-01, -3.4062e-01],\n",
      "        [ 6.0510e-02, -9.0577e-01],\n",
      "        [ 1.9762e-01, -5.2680e-01],\n",
      "        [ 1.5253e-01, -4.7782e-01],\n",
      "        [ 4.2391e-01, -7.0199e-01],\n",
      "        [ 3.6519e-02, -5.2976e-01],\n",
      "        [ 1.0694e-01, -3.2021e-01],\n",
      "        [-1.3579e-01, -2.9257e-01],\n",
      "        [ 1.2719e-01, -8.5621e-01],\n",
      "        [-4.3946e-02, -6.7028e-01],\n",
      "        [ 1.9519e-04, -7.5513e-01],\n",
      "        [ 1.7530e-01, -6.9121e-01],\n",
      "        [ 1.9018e-01, -4.1401e-01],\n",
      "        [-1.6888e-01, -1.0782e+00],\n",
      "        [ 3.0204e-01, -2.2910e-01],\n",
      "        [-1.3539e-01, -1.0076e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-9.1527e-03, -1.9178e-01],\n",
      "        [-1.1590e-01, -5.9243e-01],\n",
      "        [-1.2903e-01, -8.8134e-01],\n",
      "        [ 3.9538e-01, -2.0405e-01],\n",
      "        [ 1.0462e-01, -7.7862e-01],\n",
      "        [ 1.2422e-01, -4.8641e-01],\n",
      "        [-4.1232e-02,  9.6331e-02],\n",
      "        [ 1.2578e-01, -5.4690e-01],\n",
      "        [-1.3753e-01, -4.6166e-01],\n",
      "        [-2.7769e-01, -9.6874e-01],\n",
      "        [ 1.7451e-01, -1.0611e-01],\n",
      "        [ 4.6310e-01, -7.6872e-01],\n",
      "        [-4.8314e-02, -1.1903e-01],\n",
      "        [-6.2969e-02, -8.7289e-01],\n",
      "        [ 7.0111e-01, -3.3628e-01],\n",
      "        [ 3.0703e-02, -9.2993e-01],\n",
      "        [-1.1315e-01, -4.4642e-01],\n",
      "        [ 3.4536e-01, -9.1026e-01],\n",
      "        [ 4.8436e-01, -4.4774e-01],\n",
      "        [-1.7327e-02, -1.0045e+00],\n",
      "        [ 2.5881e-01, -7.7280e-01],\n",
      "        [ 1.0149e-01, -4.7556e-02],\n",
      "        [ 3.4382e-01, -6.8666e-01],\n",
      "        [ 1.8390e-01,  7.4917e-02],\n",
      "        [-1.4260e-01, -3.7630e-01],\n",
      "        [ 1.1399e-01, -9.0832e-01],\n",
      "        [ 4.0792e-01, -1.1369e+00],\n",
      "        [ 1.1015e-01, -5.6474e-01],\n",
      "        [ 1.1549e-01, -6.6740e-01],\n",
      "        [-3.6388e-02, -1.2851e-01],\n",
      "        [ 1.0469e-01, -9.2835e-01],\n",
      "        [ 2.9015e-01, -3.0588e-01],\n",
      "        [ 1.2130e-01, -2.8522e-01],\n",
      "        [-9.3291e-03, -6.0147e-01],\n",
      "        [ 1.4173e-01, -7.7782e-01],\n",
      "        [ 1.1731e-01, -7.1902e-01],\n",
      "        [ 3.6342e-01, -3.5863e-01],\n",
      "        [ 7.1787e-03, -1.0941e+00],\n",
      "        [ 1.6200e-01, -5.7253e-01],\n",
      "        [ 3.4519e-01, -3.3634e-01],\n",
      "        [-3.5795e-01, -7.2653e-01],\n",
      "        [ 4.7062e-01, -3.9472e-01],\n",
      "        [-3.7660e-02, -7.0037e-01],\n",
      "        [ 2.3779e-02, -6.5345e-01],\n",
      "        [-2.4339e-02, -8.8639e-01],\n",
      "        [ 2.7151e-01, -3.1413e-01],\n",
      "        [ 1.2851e-01, -1.0106e+00],\n",
      "        [-3.2922e-02, -2.4011e-01],\n",
      "        [-1.9622e-01, -4.9284e-01],\n",
      "        [ 2.7864e-01, -7.5694e-01],\n",
      "        [-1.0158e-03, -4.3013e-01],\n",
      "        [-6.2851e-02, -5.6648e-01],\n",
      "        [ 3.3746e-01, -8.6234e-01],\n",
      "        [-1.9123e-02, -2.4709e-01],\n",
      "        [-2.9024e-01, -5.5825e-01],\n",
      "        [ 3.9218e-01, -9.2804e-01],\n",
      "        [ 3.0716e-01, -4.3352e-01],\n",
      "        [ 3.8192e-01, -3.7951e-01],\n",
      "        [ 9.0415e-02, -9.0294e-01],\n",
      "        [ 1.1964e-02, -2.4790e-01],\n",
      "        [ 2.3154e-01, -8.4421e-01],\n",
      "        [ 4.9386e-01, -9.2346e-01],\n",
      "        [-8.9095e-02, -4.7934e-01],\n",
      "        [ 6.8730e-01, -7.5865e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-0.0888, -1.0186],\n",
      "        [ 0.5556, -0.9323],\n",
      "        [ 0.1897, -0.6751],\n",
      "        [-0.0324, -0.4172],\n",
      "        [ 0.3516, -0.4602],\n",
      "        [-0.0237, -0.2162],\n",
      "        [ 0.2690,  0.2221],\n",
      "        [ 0.2132, -0.6263],\n",
      "        [-0.1461, -0.8500],\n",
      "        [ 0.0528, -0.5431],\n",
      "        [ 0.0786, -0.4140],\n",
      "        [ 0.5699, -0.6080],\n",
      "        [-0.2208, -0.5460],\n",
      "        [ 0.1965, -0.7955],\n",
      "        [-0.0822, -0.1983],\n",
      "        [ 0.1289, -0.5664],\n",
      "        [ 0.0947, -0.1240],\n",
      "        [-0.1068, -0.9232],\n",
      "        [-0.2452, -0.1490],\n",
      "        [ 0.2236, -0.6099],\n",
      "        [ 0.3600, -0.7034],\n",
      "        [ 0.0086, -0.8030],\n",
      "        [ 0.2054,  0.0982],\n",
      "        [-0.0484, -0.5478],\n",
      "        [ 0.4045, -0.6503],\n",
      "        [-0.1815, -0.4471],\n",
      "        [-0.0927, -1.0396],\n",
      "        [-0.0586, -0.2828],\n",
      "        [ 0.2745, -0.2313],\n",
      "        [ 0.5362, -0.3690],\n",
      "        [ 0.2935, -0.6447],\n",
      "        [ 0.2493, -0.4704],\n",
      "        [ 0.0176, -0.3288],\n",
      "        [ 0.3250, -0.5777],\n",
      "        [ 0.1206,  0.1223],\n",
      "        [-0.0560, -0.4403],\n",
      "        [-0.2141, -0.3429],\n",
      "        [-0.1729, -0.2294],\n",
      "        [ 0.2162, -0.7331],\n",
      "        [-0.1804, -0.8289],\n",
      "        [ 0.1300, -0.2007],\n",
      "        [-0.1824, -0.4024],\n",
      "        [ 0.3149, -0.7317],\n",
      "        [ 0.4556, -0.1998],\n",
      "        [ 0.1241, -0.2708],\n",
      "        [-0.0748, -0.3846],\n",
      "        [ 0.3994, -0.1133],\n",
      "        [ 0.1247, -0.5530],\n",
      "        [-0.1293, -0.5345],\n",
      "        [-0.1206, -0.7619],\n",
      "        [ 0.0688, -0.9416],\n",
      "        [ 0.6321, -0.3945],\n",
      "        [ 0.0599, -0.2742],\n",
      "        [ 0.4348, -0.4759],\n",
      "        [ 0.0127, -0.5029],\n",
      "        [ 0.5907, -0.7344],\n",
      "        [ 0.2046, -0.1364],\n",
      "        [ 0.0030, -0.7076],\n",
      "        [-0.2343, -0.1281],\n",
      "        [-0.0071, -1.1289],\n",
      "        [-0.1540, -0.3230],\n",
      "        [ 0.2902, -0.7326],\n",
      "        [ 0.1481, -0.2268],\n",
      "        [ 0.6228, -0.4339]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[ 1.6738e-01, -5.5274e-01],\n",
      "        [-2.2925e-01, -3.1009e-01],\n",
      "        [-9.4894e-02, -4.1956e-01],\n",
      "        [-5.5213e-02, -7.9839e-01],\n",
      "        [ 5.8821e-01, -9.3509e-01],\n",
      "        [ 7.9310e-02, -6.3206e-01],\n",
      "        [ 7.2530e-02, -2.0996e-01],\n",
      "        [ 3.2001e-01, -9.5028e-01],\n",
      "        [ 2.7412e-01, -1.9097e-01],\n",
      "        [-5.1932e-02, -6.7665e-01],\n",
      "        [ 1.6605e-01, -6.7085e-01],\n",
      "        [-2.6416e-01, -6.0616e-01],\n",
      "        [-1.7062e-01, -3.7491e-01],\n",
      "        [-2.0876e-01, -4.7617e-01],\n",
      "        [-1.6289e-01, -2.2733e-01],\n",
      "        [ 5.9495e-01, -6.6692e-01],\n",
      "        [ 1.9110e-01, -4.1431e-01],\n",
      "        [ 4.7548e-01, -6.4936e-01],\n",
      "        [ 3.5183e-01,  9.9565e-02],\n",
      "        [ 3.6265e-01, -2.4550e-01],\n",
      "        [ 2.3908e-02, -7.1906e-01],\n",
      "        [-2.3360e-02, -8.1942e-01],\n",
      "        [-1.4091e-01, -6.8541e-01],\n",
      "        [ 2.9205e-01, -6.9871e-01],\n",
      "        [ 3.9079e-01, -1.1964e+00],\n",
      "        [ 4.2360e-01, -5.0537e-01],\n",
      "        [ 8.3824e-04, -6.7822e-01],\n",
      "        [ 2.8829e-01, -1.2381e+00],\n",
      "        [-4.4803e-01, -4.4792e-01],\n",
      "        [-7.7912e-02, -6.9521e-01],\n",
      "        [ 2.9998e-03, -5.4672e-01],\n",
      "        [ 1.7849e-01, -3.5514e-01],\n",
      "        [ 1.9765e-02, -1.2056e-01],\n",
      "        [ 2.1236e-01, -8.5349e-01],\n",
      "        [ 2.9967e-01, -7.4561e-01],\n",
      "        [ 9.2343e-02, -3.3349e-01],\n",
      "        [ 1.6919e-01, -3.7244e-01],\n",
      "        [ 4.1485e-01,  1.3264e-01],\n",
      "        [ 8.1746e-01, -9.3097e-01],\n",
      "        [ 5.5570e-01, -1.5163e-01],\n",
      "        [ 2.7767e-02, -3.9961e-01],\n",
      "        [ 8.8637e-02, -3.0295e-01],\n",
      "        [ 9.7326e-02, -6.3670e-01],\n",
      "        [ 1.8186e-01, -7.7470e-01],\n",
      "        [ 5.6243e-01, -5.5758e-01],\n",
      "        [ 9.1569e-02, -6.3848e-01],\n",
      "        [ 1.4069e-01, -6.3465e-01],\n",
      "        [-1.7916e-02, -1.2590e-01],\n",
      "        [ 8.9315e-02,  7.8887e-04],\n",
      "        [ 4.6429e-02, -5.9970e-01],\n",
      "        [-3.2026e-01, -7.7309e-01],\n",
      "        [ 3.9256e-01, -1.9538e-01],\n",
      "        [-9.3803e-02, -5.0675e-01],\n",
      "        [ 4.6823e-01, -6.5728e-01],\n",
      "        [-1.1446e-01, -4.9633e-01],\n",
      "        [ 2.4853e-01,  1.3471e-01],\n",
      "        [ 4.4284e-02, -7.0197e-01],\n",
      "        [-2.8708e-01, -5.3522e-01],\n",
      "        [ 2.1512e-01, -6.9765e-01],\n",
      "        [ 4.3570e-01, -9.1337e-02],\n",
      "        [-3.6939e-02, -1.9378e-01],\n",
      "        [ 1.7423e-01,  2.4774e-01],\n",
      "        [-8.7059e-03, -3.9982e-02],\n",
      "        [ 1.0611e-01, -5.0990e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[ 1.7150e-01, -1.8254e-01],\n",
      "        [ 6.1890e-01, -1.0751e+00],\n",
      "        [-7.8574e-02, -4.1881e-01],\n",
      "        [-2.1349e-01, -7.5148e-01],\n",
      "        [ 9.2915e-02, -8.7967e-02],\n",
      "        [ 4.3540e-01, -5.0481e-01],\n",
      "        [-1.8632e-01, -4.7236e-01],\n",
      "        [ 1.0782e-01, -1.3708e-01],\n",
      "        [-1.0799e-02, -1.3134e+00],\n",
      "        [ 3.0465e-02, -4.5144e-01],\n",
      "        [ 6.8021e-01, -8.5524e-01],\n",
      "        [-2.4355e-02, -2.0468e-01],\n",
      "        [-1.5491e-01, -8.0624e-01],\n",
      "        [ 8.9156e-02, -7.3254e-01],\n",
      "        [-2.7253e-01, -3.8879e-01],\n",
      "        [ 1.3779e-01, -9.5220e-01],\n",
      "        [-2.0691e-01, -4.2409e-01],\n",
      "        [-4.0506e-02, -2.7866e-01],\n",
      "        [-1.6475e-01, -5.2186e-01],\n",
      "        [ 1.9119e-01, -6.2103e-01],\n",
      "        [ 7.3872e-04, -2.7850e-01],\n",
      "        [ 8.7359e-02, -5.6820e-01],\n",
      "        [-2.5663e-01, -9.2209e-01],\n",
      "        [-4.5597e-02, -6.7725e-01],\n",
      "        [ 2.2667e-02, -3.9528e-01],\n",
      "        [-2.2536e-01, -4.4816e-01],\n",
      "        [ 2.9653e-01, -1.0808e+00],\n",
      "        [-2.7119e-02,  5.8897e-03],\n",
      "        [ 3.5978e-01, -4.9787e-01],\n",
      "        [-1.7038e-01, -2.3183e-01],\n",
      "        [ 1.8563e-01, -2.2535e-01],\n",
      "        [-1.9177e-01, -8.4954e-02],\n",
      "        [-3.8811e-02, -4.3219e-01],\n",
      "        [-1.0068e-01, -1.5416e-01],\n",
      "        [ 5.3869e-01, -9.4853e-01],\n",
      "        [ 5.1205e-01, -6.8430e-01],\n",
      "        [ 2.6316e-01, -1.1887e-01],\n",
      "        [ 2.8509e-02, -4.1041e-02],\n",
      "        [ 2.5527e-01, -4.1898e-01],\n",
      "        [ 1.5421e-01,  1.5545e-01],\n",
      "        [-3.4698e-02, -4.2362e-01],\n",
      "        [ 1.0633e-01, -2.8810e-01],\n",
      "        [ 4.1401e-02, -5.4029e-01],\n",
      "        [-3.6339e-02, -2.2375e-01],\n",
      "        [ 2.9128e-01, -8.9981e-01],\n",
      "        [-7.0953e-03, -4.0504e-01],\n",
      "        [-1.8454e-02, -8.6968e-01],\n",
      "        [-4.4666e-02, -4.0207e-01],\n",
      "        [ 6.4852e-01, -1.0096e+00],\n",
      "        [ 3.0258e-01, -5.5318e-01],\n",
      "        [ 4.7493e-01, -4.0318e-01],\n",
      "        [-8.2606e-02, -2.5824e-02],\n",
      "        [-1.4484e-02, -4.2106e-01],\n",
      "        [-1.1080e-01, -5.5820e-01],\n",
      "        [ 2.6831e-02, -4.4292e-01],\n",
      "        [ 4.2348e-01, -4.2847e-01],\n",
      "        [ 1.3200e-01, -1.7526e-01],\n",
      "        [ 1.6874e-01, -2.1218e-01],\n",
      "        [ 1.0454e-02, -4.0902e-01],\n",
      "        [-9.1413e-02, -6.0668e-01],\n",
      "        [-2.2663e-02, -5.3175e-01],\n",
      "        [-1.5062e-01, -8.5051e-01],\n",
      "        [ 2.5594e-01, -1.7051e-01],\n",
      "        [ 6.0670e-02, -7.2872e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-0.0882, -0.8951],\n",
      "        [-0.0410, -0.3357],\n",
      "        [ 0.3225, -0.7097],\n",
      "        [-0.1092, -1.0292],\n",
      "        [ 0.3248, -0.1385],\n",
      "        [ 0.0621, -0.5090],\n",
      "        [ 0.1406, -0.8307],\n",
      "        [ 0.0112, -0.4803],\n",
      "        [-0.2048, -0.5143],\n",
      "        [ 0.0555, -0.7672],\n",
      "        [-0.2742, -0.9540],\n",
      "        [ 0.2599, -0.3452],\n",
      "        [ 0.4007, -0.0975],\n",
      "        [-0.2594, -0.7379],\n",
      "        [-0.4432, -1.2080],\n",
      "        [ 0.0811, -0.1461],\n",
      "        [ 0.0813, -0.3201],\n",
      "        [ 0.1619, -0.3364],\n",
      "        [ 0.0575, -0.4544],\n",
      "        [ 0.0184, -0.6329],\n",
      "        [ 0.1775, -0.0676],\n",
      "        [ 0.0028, -0.1576],\n",
      "        [-0.3979, -0.6443],\n",
      "        [-0.0303, -0.0502],\n",
      "        [-0.0108,  0.0566],\n",
      "        [ 0.0700,  0.1989],\n",
      "        [ 0.0707, -0.7828],\n",
      "        [ 0.1797, -0.6969],\n",
      "        [-0.0449, -0.1455],\n",
      "        [ 0.1845, -0.1654],\n",
      "        [ 0.0128, -0.1943],\n",
      "        [-0.1342, -0.2925],\n",
      "        [-0.0787, -0.3385],\n",
      "        [-0.1749, -0.5702],\n",
      "        [ 0.2237, -0.4542],\n",
      "        [-0.3902, -0.5263],\n",
      "        [ 0.3456, -0.1620],\n",
      "        [ 0.0440, -0.3453],\n",
      "        [ 0.2814, -0.8683],\n",
      "        [-0.2015, -0.3989],\n",
      "        [ 0.2451, -0.4489],\n",
      "        [-0.1009, -0.2707],\n",
      "        [-0.2523, -0.5174],\n",
      "        [ 0.0092, -0.6864],\n",
      "        [-0.0730, -0.6141],\n",
      "        [ 0.1231, -0.5989],\n",
      "        [-0.0494, -0.8780],\n",
      "        [ 0.1064, -0.3908],\n",
      "        [ 0.4120, -0.6425],\n",
      "        [ 0.0594, -0.4243],\n",
      "        [-0.0707, -0.8924],\n",
      "        [ 0.1509, -0.5137],\n",
      "        [ 0.1203, -0.4097],\n",
      "        [ 0.0652, -0.1266],\n",
      "        [ 0.3935, -0.0855],\n",
      "        [ 0.1689, -0.4949],\n",
      "        [-0.0813, -0.0898],\n",
      "        [-0.0345, -0.3947],\n",
      "        [ 0.0270, -0.0915],\n",
      "        [ 0.3639, -0.3939],\n",
      "        [-0.3013, -0.4678],\n",
      "        [-0.1406, -0.3932],\n",
      "        [ 0.2810, -0.5152],\n",
      "        [-0.0426, -0.4886]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[ 0.0076, -0.2936],\n",
      "        [-0.1990, -0.5753],\n",
      "        [ 0.0687, -0.5876],\n",
      "        [ 0.4041, -0.9877],\n",
      "        [-0.1642, -0.2552],\n",
      "        [-0.0268, -0.5841],\n",
      "        [-0.2561, -0.2091],\n",
      "        [ 0.4649, -0.4607],\n",
      "        [ 0.1399, -0.0060],\n",
      "        [ 0.0094, -0.2754],\n",
      "        [ 0.2554, -0.8387],\n",
      "        [ 0.5720, -0.2940],\n",
      "        [-0.2174, -0.1989],\n",
      "        [-0.2413, -0.1565],\n",
      "        [ 0.5018, -0.6642],\n",
      "        [ 0.5663, -1.0549],\n",
      "        [ 0.1724, -0.5320],\n",
      "        [ 0.3101, -0.3601],\n",
      "        [ 0.5763, -0.8624],\n",
      "        [-0.1951, -0.3161],\n",
      "        [ 0.0545, -0.3623],\n",
      "        [-0.0619,  0.0310],\n",
      "        [ 0.0374, -0.1439],\n",
      "        [ 0.0256, -0.8157],\n",
      "        [-0.2089, -0.5880],\n",
      "        [-0.0827, -0.3961],\n",
      "        [-0.0475, -0.5093],\n",
      "        [-0.0705, -0.4572],\n",
      "        [-0.1424, -0.1712],\n",
      "        [-0.2153, -0.7205],\n",
      "        [ 0.2257,  0.0758],\n",
      "        [ 0.0878,  0.0791],\n",
      "        [ 0.1115, -0.9826],\n",
      "        [ 0.2143, -0.3021],\n",
      "        [ 0.2675, -0.9380],\n",
      "        [-0.0044, -0.7352],\n",
      "        [-0.0577, -0.4885],\n",
      "        [ 0.0017, -0.4211],\n",
      "        [-0.0194, -0.3193],\n",
      "        [ 0.2153, -0.5245],\n",
      "        [-0.4192, -0.6324],\n",
      "        [-0.3765, -0.7207],\n",
      "        [ 0.0595, -0.8058],\n",
      "        [-0.0160, -0.1322],\n",
      "        [-0.0339, -0.5765],\n",
      "        [ 0.3820, -0.2992],\n",
      "        [-0.2791, -0.7762],\n",
      "        [-0.2717, -0.2765],\n",
      "        [ 0.0058, -0.5637],\n",
      "        [-0.2747, -0.5888],\n",
      "        [ 0.0920, -0.4328],\n",
      "        [ 0.2637, -0.6154],\n",
      "        [-0.1081, -0.0849],\n",
      "        [ 0.7583, -0.9060],\n",
      "        [ 0.3537, -0.8553],\n",
      "        [-0.2537, -0.1594],\n",
      "        [ 0.0778, -1.0516],\n",
      "        [-0.1183, -0.0289],\n",
      "        [-0.2457, -1.0412],\n",
      "        [ 0.0218, -0.6208],\n",
      "        [ 0.0976, -1.0862],\n",
      "        [ 0.4097, -0.6501],\n",
      "        [ 0.0802,  0.1099],\n",
      "        [-0.0429, -0.4480]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-0.1906, -0.1870],\n",
      "        [ 0.2158, -0.1161],\n",
      "        [-0.4427, -0.1759],\n",
      "        [ 0.2151, -1.1508],\n",
      "        [ 0.2498, -0.5694],\n",
      "        [ 0.2632, -0.5504],\n",
      "        [ 0.0142, -0.1441],\n",
      "        [ 0.1102, -0.6641],\n",
      "        [ 0.0350, -0.4100],\n",
      "        [ 0.1158, -0.8869],\n",
      "        [-0.1710, -0.3093],\n",
      "        [-0.0419, -0.8395],\n",
      "        [-0.0291, -0.1693],\n",
      "        [-0.1304, -0.3436],\n",
      "        [-0.2210, -0.6844],\n",
      "        [ 0.0633, -0.5411],\n",
      "        [ 0.6791, -0.1749],\n",
      "        [ 0.3683, -0.4319],\n",
      "        [-0.0767, -0.7776],\n",
      "        [ 0.1834, -0.2914],\n",
      "        [-0.0736, -0.5481],\n",
      "        [-0.0459, -0.1159],\n",
      "        [ 0.1673, -1.1806],\n",
      "        [ 0.0453,  0.0357],\n",
      "        [-0.3227, -0.1714],\n",
      "        [ 0.3380, -0.1534],\n",
      "        [-0.0278, -0.1795],\n",
      "        [ 0.2108, -0.5736],\n",
      "        [-0.0751, -0.2373],\n",
      "        [-0.1406, -0.9299],\n",
      "        [ 0.5594, -0.8565],\n",
      "        [-0.4751, -0.3093],\n",
      "        [ 0.4854, -0.8306],\n",
      "        [-0.2786, -0.3220],\n",
      "        [-0.3073, -0.4083],\n",
      "        [ 0.4602, -0.3927],\n",
      "        [-0.2192, -0.6983],\n",
      "        [-0.0109, -0.3434],\n",
      "        [ 0.0447, -0.2321],\n",
      "        [ 0.0877, -0.4210],\n",
      "        [ 0.2957, -0.3354],\n",
      "        [-0.0330, -0.2847],\n",
      "        [-0.0243, -0.2517],\n",
      "        [-0.1432, -0.4822],\n",
      "        [-0.1312, -0.2762],\n",
      "        [ 0.1405, -0.7887],\n",
      "        [-0.0216, -0.3768],\n",
      "        [ 0.0472, -0.1307],\n",
      "        [ 0.1939, -0.7169],\n",
      "        [-0.2634, -0.0320],\n",
      "        [-0.1580, -0.4504],\n",
      "        [-0.0383, -0.6489],\n",
      "        [-0.1363, -0.4331],\n",
      "        [ 0.0426, -0.0450],\n",
      "        [ 0.5856, -0.5670],\n",
      "        [-0.1864, -0.8042],\n",
      "        [ 0.1522,  0.0641],\n",
      "        [-0.2275, -0.9641],\n",
      "        [-0.0221, -0.3598],\n",
      "        [-0.0700,  0.0737],\n",
      "        [ 0.2745, -0.7502],\n",
      "        [-0.0395, -0.6808],\n",
      "        [-0.0860, -0.6473],\n",
      "        [ 0.0106, -0.8344]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[ 0.0641, -0.7277],\n",
      "        [-0.1077, -0.6511],\n",
      "        [-0.3334, -0.4846],\n",
      "        [ 0.2631,  0.0089],\n",
      "        [ 0.0362, -0.2256],\n",
      "        [ 0.1289, -0.5452],\n",
      "        [-0.3618,  0.0281],\n",
      "        [ 0.3283, -0.9380],\n",
      "        [-0.2355, -0.3319],\n",
      "        [-0.1299, -0.6376],\n",
      "        [-0.1250, -0.2747],\n",
      "        [-0.2252,  0.2027],\n",
      "        [-0.0322, -0.0755],\n",
      "        [ 0.0727, -0.6062],\n",
      "        [ 0.1252, -0.6044],\n",
      "        [-0.0740,  0.0653],\n",
      "        [ 0.0913, -0.8143],\n",
      "        [ 0.0390, -0.5450],\n",
      "        [-0.1218, -0.9285],\n",
      "        [-0.1627, -0.0632],\n",
      "        [ 0.1202, -0.3207],\n",
      "        [ 0.4155, -1.0180],\n",
      "        [-0.4694, -0.6221],\n",
      "        [-0.2475, -0.4162],\n",
      "        [-0.2545, -0.6197],\n",
      "        [-0.0713, -0.0458],\n",
      "        [-0.0900, -0.2681],\n",
      "        [ 0.1711, -0.3262],\n",
      "        [-0.1064,  0.1331],\n",
      "        [-0.3140, -0.5742],\n",
      "        [-0.2471, -0.0656],\n",
      "        [ 0.0536, -0.0482],\n",
      "        [-0.1139, -0.4304],\n",
      "        [-0.0676, -0.1176],\n",
      "        [ 0.3240, -0.4328],\n",
      "        [ 0.0514, -0.2638],\n",
      "        [ 0.1983, -0.7375],\n",
      "        [ 0.2423, -0.4276],\n",
      "        [ 0.1962, -0.8511],\n",
      "        [-0.0734, -0.4902],\n",
      "        [ 0.0217, -0.6670],\n",
      "        [-0.1264, -0.5717],\n",
      "        [ 0.1687, -0.8804],\n",
      "        [ 0.2519, -0.1804],\n",
      "        [ 0.0313, -0.2469],\n",
      "        [-0.2515, -0.1221],\n",
      "        [ 0.0299, -0.5259],\n",
      "        [ 0.0116, -0.8366],\n",
      "        [ 0.0296, -0.4934],\n",
      "        [-0.2542, -0.3191],\n",
      "        [-0.0583, -0.4000],\n",
      "        [ 0.1363, -0.1422],\n",
      "        [ 0.0549, -0.4519],\n",
      "        [ 0.0670, -0.6089],\n",
      "        [-0.1083, -0.4023],\n",
      "        [ 0.0222, -0.6060],\n",
      "        [ 0.0834, -0.6149],\n",
      "        [-0.1082, -0.0811],\n",
      "        [ 0.1085, -0.6090],\n",
      "        [ 0.0321, -0.4329],\n",
      "        [ 0.1939, -0.3396],\n",
      "        [ 0.2243, -0.5846],\n",
      "        [ 0.2063, -0.3288],\n",
      "        [ 0.1908, -0.6523]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[ 0.2253, -0.4547],\n",
      "        [-0.0406, -0.5225],\n",
      "        [ 0.3246, -0.2962],\n",
      "        [ 0.2075, -0.5356],\n",
      "        [-0.3722, -0.3298],\n",
      "        [-0.1700, -0.2893],\n",
      "        [-0.3050, -0.7312],\n",
      "        [ 0.1599, -0.3151],\n",
      "        [ 0.0083, -1.2470],\n",
      "        [-0.3779, -0.0577],\n",
      "        [-0.0348, -0.2337],\n",
      "        [ 0.2606, -0.3624],\n",
      "        [ 0.1152, -0.1125],\n",
      "        [-0.2511, -0.7266],\n",
      "        [-0.0015, -0.4988],\n",
      "        [ 0.2730, -0.8859],\n",
      "        [-0.2960, -0.6331],\n",
      "        [-0.2399, -0.3980],\n",
      "        [-0.2384, -0.5379],\n",
      "        [-0.4479,  0.0168],\n",
      "        [-0.0256, -0.0200],\n",
      "        [-0.2575, -0.1205],\n",
      "        [ 0.1122, -0.0310],\n",
      "        [ 0.1333, -0.8006],\n",
      "        [-0.1324, -0.7815],\n",
      "        [ 0.2138, -0.6757],\n",
      "        [-0.2683, -0.0045],\n",
      "        [-0.0562, -0.0198],\n",
      "        [-0.4935, -0.6899],\n",
      "        [-0.3298, -0.4734],\n",
      "        [ 0.4981, -0.5970],\n",
      "        [-0.2155, -0.5339],\n",
      "        [-0.0211, -0.6603],\n",
      "        [ 0.0904, -0.2957],\n",
      "        [ 0.2900,  0.0595],\n",
      "        [-0.4978, -0.4956],\n",
      "        [-0.2598,  0.0261],\n",
      "        [-0.0859, -0.4516],\n",
      "        [-0.1349, -0.4240],\n",
      "        [ 0.1032, -0.9184],\n",
      "        [-0.0318, -0.4090],\n",
      "        [ 0.0219, -0.4755],\n",
      "        [ 0.4168, -0.1155],\n",
      "        [ 0.0972, -0.3957],\n",
      "        [-0.1866, -0.2116],\n",
      "        [-0.3092, -0.4319],\n",
      "        [-0.5768, -0.4435],\n",
      "        [-0.0209, -0.3714],\n",
      "        [-0.2588, -0.5669],\n",
      "        [ 0.1660, -0.0727],\n",
      "        [ 0.4847, -0.2645],\n",
      "        [ 0.0794, -0.8505],\n",
      "        [ 0.1108, -0.5248],\n",
      "        [-0.0846, -0.2355],\n",
      "        [-0.0194, -0.1334],\n",
      "        [-0.0414, -0.2691],\n",
      "        [-0.1322, -0.7302],\n",
      "        [-0.2718, -0.3194],\n",
      "        [ 0.2431, -0.6900],\n",
      "        [-0.2885, -0.8219],\n",
      "        [ 0.1019, -0.2922],\n",
      "        [ 0.1386, -0.0865],\n",
      "        [-0.1660, -0.5458],\n",
      "        [-0.1293, -0.4809]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-0.1271,  0.1694],\n",
      "        [-0.1964, -0.0999],\n",
      "        [-0.0475,  0.0332],\n",
      "        [-0.2305, -0.5702],\n",
      "        [-0.2182, -0.3300],\n",
      "        [-0.2642, -0.2166],\n",
      "        [-0.2916, -0.3151],\n",
      "        [ 0.0754, -0.3652],\n",
      "        [-0.4120, -0.4124],\n",
      "        [ 0.2978, -0.4803],\n",
      "        [-0.1912, -0.6461],\n",
      "        [-0.0721, -0.5231],\n",
      "        [ 0.2110, -0.2599],\n",
      "        [ 0.3387, -0.4739],\n",
      "        [-0.2027, -0.8298],\n",
      "        [-0.1507, -0.6993],\n",
      "        [ 0.0118, -0.2748],\n",
      "        [-0.0114, -0.0639],\n",
      "        [-0.3812, -0.0597],\n",
      "        [-0.2527, -0.7123],\n",
      "        [-0.2263, -0.0270],\n",
      "        [-0.1913, -0.0227],\n",
      "        [ 0.0044,  0.1096],\n",
      "        [-0.0381, -0.4311],\n",
      "        [ 0.0969, -0.6148],\n",
      "        [-0.1528, -0.2978],\n",
      "        [-0.1014,  0.0826],\n",
      "        [ 0.1660,  0.1110],\n",
      "        [-0.2051, -0.4465],\n",
      "        [ 0.0018, -0.2057],\n",
      "        [-0.2960, -0.7215],\n",
      "        [-0.0576, -0.4030],\n",
      "        [ 0.0103, -0.3480],\n",
      "        [-0.1031,  0.1773],\n",
      "        [-0.1156, -0.1479],\n",
      "        [ 0.3008, -0.3512],\n",
      "        [-0.2503, -0.1815],\n",
      "        [-0.0069,  0.3339],\n",
      "        [-0.1751, -0.3655],\n",
      "        [-0.1294, -0.1806],\n",
      "        [-0.3366, -0.6189],\n",
      "        [-0.2506, -0.0779],\n",
      "        [-0.0606, -0.2572],\n",
      "        [ 0.0685,  0.0380],\n",
      "        [ 0.0390, -0.7121],\n",
      "        [ 0.3581, -1.0617],\n",
      "        [-0.0273, -0.1486],\n",
      "        [-0.1710, -0.0402],\n",
      "        [-0.0980, -0.4310],\n",
      "        [ 0.0492, -0.4817],\n",
      "        [ 0.0114, -0.5210],\n",
      "        [ 0.0776, -0.2180],\n",
      "        [ 0.3404, -0.6091],\n",
      "        [-0.3082, -0.6808],\n",
      "        [ 0.1984,  0.1265],\n",
      "        [ 0.0849, -0.5891],\n",
      "        [-0.0400, -0.5961],\n",
      "        [-0.3292, -0.5603],\n",
      "        [-0.1165, -0.4237],\n",
      "        [ 0.0913, -0.7387],\n",
      "        [-0.0621, -0.3648],\n",
      "        [ 0.0332, -0.0611],\n",
      "        [ 0.1926,  0.1141],\n",
      "        [-0.3264, -0.2207]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-0.0087, -0.4775],\n",
      "        [ 0.0674, -0.9221],\n",
      "        [-0.3120, -0.5906],\n",
      "        [ 0.0982, -0.7668],\n",
      "        [ 0.1014, -0.1190],\n",
      "        [-0.0182, -0.5424],\n",
      "        [-0.4375, -0.4149],\n",
      "        [ 0.1454, -0.6487],\n",
      "        [-0.0729, -0.5176],\n",
      "        [ 0.0383, -0.1463],\n",
      "        [-0.1755, -0.4142],\n",
      "        [-0.3738, -0.1241],\n",
      "        [-0.3071,  0.1205],\n",
      "        [ 0.1284, -0.2436],\n",
      "        [-0.1219,  0.0459],\n",
      "        [ 0.0735, -0.2838],\n",
      "        [ 0.0800, -0.4723],\n",
      "        [ 0.3207, -0.5545],\n",
      "        [ 0.0361,  0.0511],\n",
      "        [-0.2115,  0.3188],\n",
      "        [-0.4815, -0.6958],\n",
      "        [-0.3110, -0.4087],\n",
      "        [-0.1232, -0.0381],\n",
      "        [-0.3342, -0.1790],\n",
      "        [ 0.1519, -0.6129],\n",
      "        [-0.0775, -0.2920],\n",
      "        [ 0.0979,  0.1835],\n",
      "        [-0.4223, -0.2198],\n",
      "        [-0.1385, -0.4326],\n",
      "        [-0.0804, -0.4119],\n",
      "        [-0.1413, -1.0809],\n",
      "        [-0.0236,  0.1325],\n",
      "        [ 0.0173, -0.1339],\n",
      "        [-0.2794, -0.8752],\n",
      "        [-0.3554, -0.8271],\n",
      "        [-0.0498, -0.1185],\n",
      "        [-0.2758, -0.1060],\n",
      "        [-0.2077, -0.8054],\n",
      "        [-0.4029, -0.0670],\n",
      "        [ 0.1256, -0.7947],\n",
      "        [ 0.1040, -0.4765],\n",
      "        [-0.1700, -0.1537],\n",
      "        [-0.2057, -0.0424],\n",
      "        [-0.0793, -0.8446],\n",
      "        [-0.1067, -0.0993],\n",
      "        [-0.1691, -0.4786],\n",
      "        [-0.1940, -0.2521],\n",
      "        [-0.3984,  0.0473],\n",
      "        [-0.1667, -0.6352],\n",
      "        [-0.0022,  0.0148],\n",
      "        [-0.1363, -0.1122],\n",
      "        [-0.1703, -0.1312],\n",
      "        [-0.1518, -0.0734],\n",
      "        [-0.1346, -0.5422],\n",
      "        [ 0.0948,  0.2467],\n",
      "        [ 0.0135, -0.5142],\n",
      "        [-0.0138, -0.2003],\n",
      "        [ 0.2066, -0.7524],\n",
      "        [ 0.0395, -0.4450],\n",
      "        [ 0.0111, -0.5482],\n",
      "        [-0.0808, -0.1875],\n",
      "        [-0.3127, -0.3604],\n",
      "        [-0.0787,  0.0941],\n",
      "        [-0.3177, -0.8287]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-3.6816e-01, -7.7745e-01],\n",
      "        [-1.2193e-01, -3.4633e-01],\n",
      "        [ 4.5151e-01,  6.1724e-01],\n",
      "        [ 9.5700e-02, -2.2416e-01],\n",
      "        [-2.5606e-01, -3.5790e-01],\n",
      "        [ 6.7858e-02, -4.7923e-02],\n",
      "        [-3.5339e-01, -6.2599e-01],\n",
      "        [-6.9666e-02, -4.1805e-02],\n",
      "        [-1.5422e-02, -1.4085e-02],\n",
      "        [-5.9982e-01, -5.3762e-01],\n",
      "        [-4.6949e-02,  1.6166e-01],\n",
      "        [-6.6895e-02, -3.3094e-01],\n",
      "        [-1.6030e-01,  2.0898e-01],\n",
      "        [-3.5287e-02, -7.6172e-02],\n",
      "        [-1.6359e-01, -6.8347e-01],\n",
      "        [ 2.7052e-01, -4.5204e-01],\n",
      "        [-2.3844e-03, -1.5846e-01],\n",
      "        [ 3.0007e-01, -3.2708e-01],\n",
      "        [-2.8566e-02, -3.0270e-01],\n",
      "        [ 3.1454e-02, -3.6875e-01],\n",
      "        [ 1.7066e-01, -7.5268e-01],\n",
      "        [-5.7103e-01, -3.6785e-01],\n",
      "        [-1.5510e-01,  4.4626e-02],\n",
      "        [-2.8925e-01, -6.4637e-02],\n",
      "        [ 3.4705e-01,  3.5537e-01],\n",
      "        [-1.7421e-01, -2.6159e-01],\n",
      "        [-3.3943e-03,  3.4800e-02],\n",
      "        [ 2.4641e-01, -6.2033e-01],\n",
      "        [-1.0797e-01,  9.7292e-02],\n",
      "        [ 1.1870e-01, -7.0754e-01],\n",
      "        [-2.9016e-01, -2.0222e-01],\n",
      "        [ 8.8133e-02, -2.7012e-01],\n",
      "        [-4.3173e-02, -2.7887e-01],\n",
      "        [-2.5670e-01,  3.5140e-01],\n",
      "        [ 1.1850e-01, -4.5918e-01],\n",
      "        [-1.9021e-02,  2.6402e-01],\n",
      "        [-2.6469e-01, -2.0609e-01],\n",
      "        [-1.7017e-01, -2.7701e-01],\n",
      "        [-2.9936e-01, -4.8574e-01],\n",
      "        [ 2.8731e-01, -4.6378e-01],\n",
      "        [-1.0126e-01, -1.1150e-01],\n",
      "        [-7.0413e-02,  7.2021e-03],\n",
      "        [-3.4966e-01, -3.8483e-01],\n",
      "        [-3.8727e-01, -1.3339e-01],\n",
      "        [-3.6011e-01, -2.5404e-01],\n",
      "        [-2.6275e-01, -5.0715e-01],\n",
      "        [-2.0246e-02, -2.0106e-01],\n",
      "        [-1.6538e-01, -5.2104e-01],\n",
      "        [-2.1792e-01, -7.5009e-02],\n",
      "        [-5.2967e-01, -5.0125e-01],\n",
      "        [-2.8046e-01, -5.3759e-01],\n",
      "        [-4.8569e-01, -4.2517e-01],\n",
      "        [-3.9870e-01, -2.1530e-01],\n",
      "        [-1.9336e-01, -6.9026e-01],\n",
      "        [-1.7518e-01, -1.5600e-01],\n",
      "        [-5.7702e-02,  1.9101e-01],\n",
      "        [-1.2106e-01, -1.2916e-01],\n",
      "        [ 4.2999e-02, -3.4912e-01],\n",
      "        [-4.8280e-06, -7.3549e-01],\n",
      "        [-8.4748e-02, -1.4649e-01],\n",
      "        [-7.6285e-02, -2.1308e-01],\n",
      "        [ 9.9073e-03, -6.7947e-01],\n",
      "        [-2.4485e-01, -7.4396e-01],\n",
      "        [-2.4985e-01, -3.9320e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-0.2214, -0.2869],\n",
      "        [-0.1918, -0.6567],\n",
      "        [-0.1871, -0.7422],\n",
      "        [ 0.2810,  0.3115],\n",
      "        [-0.2149,  0.1179],\n",
      "        [-0.1310, -0.1936],\n",
      "        [-0.1019, -0.4832],\n",
      "        [ 0.1346, -0.5631],\n",
      "        [ 0.0094, -0.2998],\n",
      "        [-0.2946, -0.3345],\n",
      "        [ 0.1686,  0.0072],\n",
      "        [ 0.0674, -0.0950],\n",
      "        [ 0.1505,  0.4596],\n",
      "        [ 0.0846, -0.6655],\n",
      "        [ 0.2626, -0.5601],\n",
      "        [-0.3177, -0.0218],\n",
      "        [-0.2392, -0.1688],\n",
      "        [ 0.2099,  0.0529],\n",
      "        [-0.1977, -0.3477],\n",
      "        [-0.1469, -0.6941],\n",
      "        [ 0.0077, -0.2898],\n",
      "        [ 0.2136,  0.0786],\n",
      "        [-0.2720, -0.2866],\n",
      "        [ 0.1731, -0.2309],\n",
      "        [-0.0534, -0.2934],\n",
      "        [-0.2664, -0.5573],\n",
      "        [-0.2022,  0.0604],\n",
      "        [-0.3455, -0.3091],\n",
      "        [-0.1211,  0.1617],\n",
      "        [ 0.3602, -1.0210],\n",
      "        [-0.1816, -0.2989],\n",
      "        [-0.1287,  0.0638],\n",
      "        [-0.4261, -0.1404],\n",
      "        [ 0.2368,  0.1026],\n",
      "        [-0.5380, -0.1317],\n",
      "        [-0.0777, -0.1001],\n",
      "        [ 0.2890, -0.6533],\n",
      "        [-0.0784, -0.5003],\n",
      "        [-0.0434, -0.1195],\n",
      "        [ 0.3131, -0.6985],\n",
      "        [-0.2763, -0.2879],\n",
      "        [-0.2831, -0.4352],\n",
      "        [-0.1367, -0.0673],\n",
      "        [-0.0201, -0.1380],\n",
      "        [-0.1192, -0.3218],\n",
      "        [ 0.0108, -0.6598],\n",
      "        [ 0.4626,  0.1255],\n",
      "        [ 0.0957,  0.2960],\n",
      "        [-0.2637, -0.1125],\n",
      "        [ 0.0350,  0.0100],\n",
      "        [-0.3455,  0.0564],\n",
      "        [-0.1153,  0.1945],\n",
      "        [-0.5386, -0.5037],\n",
      "        [-0.0611, -0.1277],\n",
      "        [-0.0394, -0.5530],\n",
      "        [-0.0189, -0.3029],\n",
      "        [ 0.0729, -0.6734],\n",
      "        [ 0.0434, -0.6952],\n",
      "        [ 0.0262,  0.0994],\n",
      "        [ 0.1298, -0.3220],\n",
      "        [-0.2564,  0.0881],\n",
      "        [-0.2430, -0.0340],\n",
      "        [-0.1478, -0.5013],\n",
      "        [ 0.1569,  0.0064]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-0.4708, -0.2721],\n",
      "        [ 0.0730,  0.0241],\n",
      "        [-0.1843, -0.0048],\n",
      "        [ 0.2776,  0.2286],\n",
      "        [-0.0575, -0.4504],\n",
      "        [-0.1441, -0.4737],\n",
      "        [ 0.3025, -0.2426],\n",
      "        [-0.1276, -0.3728],\n",
      "        [-0.0361, -0.0512],\n",
      "        [ 0.1891, -0.1770],\n",
      "        [ 0.1657,  0.4327],\n",
      "        [-0.0929, -0.3685],\n",
      "        [-0.3388, -0.0817],\n",
      "        [-0.1644, -0.4199],\n",
      "        [-0.3852, -0.2231],\n",
      "        [-0.3815, -0.1868],\n",
      "        [-0.1819, -0.3090],\n",
      "        [-0.0663, -0.8553],\n",
      "        [ 0.0378, -0.0299],\n",
      "        [-0.2669,  0.3024],\n",
      "        [-0.2786,  0.0933],\n",
      "        [ 0.3333, -0.1400],\n",
      "        [-0.2992, -0.1280],\n",
      "        [-0.1037, -0.5190],\n",
      "        [-0.1905, -0.4678],\n",
      "        [-0.0583, -0.3741],\n",
      "        [-0.2677,  0.1018],\n",
      "        [ 0.4846,  0.0738],\n",
      "        [ 0.1165,  0.0094],\n",
      "        [-0.2067, -0.1878],\n",
      "        [-0.2446, -0.2866],\n",
      "        [-0.1612, -0.2547],\n",
      "        [ 0.2965,  0.0977],\n",
      "        [-0.3835, -0.1088],\n",
      "        [ 0.0471, -0.5559],\n",
      "        [ 0.0989, -0.9531],\n",
      "        [ 0.0656, -0.4733],\n",
      "        [-0.3021, -0.5272],\n",
      "        [-0.0427,  0.2930],\n",
      "        [ 0.2709, -0.2596],\n",
      "        [-0.3657, -0.4731],\n",
      "        [ 0.0827, -0.1400],\n",
      "        [-0.1525,  0.0052],\n",
      "        [-0.0394, -0.1804],\n",
      "        [ 0.0062, -0.5886],\n",
      "        [ 0.1859, -0.2962],\n",
      "        [-0.0451, -0.4866],\n",
      "        [-0.2824, -0.2631],\n",
      "        [-0.3271, -0.7355],\n",
      "        [-0.3666, -0.1980],\n",
      "        [ 0.0614, -0.6243],\n",
      "        [-0.3856, -0.2742],\n",
      "        [ 0.1177, -0.4649],\n",
      "        [-0.3071,  0.0583],\n",
      "        [-0.2643, -0.5633],\n",
      "        [-0.2551,  0.2305],\n",
      "        [-0.4007, -0.1942],\n",
      "        [-0.4369, -0.3971],\n",
      "        [-0.4543, -0.4116],\n",
      "        [ 0.0056, -0.4727],\n",
      "        [-0.0139,  0.1806],\n",
      "        [-0.1560,  0.0114],\n",
      "        [-0.5373,  0.0191],\n",
      "        [-0.1085, -0.7226]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-4.0489e-01, -2.7734e-02],\n",
      "        [ 3.3111e-02,  1.2670e-01],\n",
      "        [-2.2256e-02,  1.2210e-01],\n",
      "        [-6.3003e-01, -4.1989e-01],\n",
      "        [-4.5095e-01, -4.7813e-01],\n",
      "        [-5.0522e-01, -2.4386e-01],\n",
      "        [ 4.9296e-01, -2.6802e-01],\n",
      "        [-2.3489e-01, -1.2089e-01],\n",
      "        [ 2.2676e-01,  1.4373e-01],\n",
      "        [ 2.2959e-02, -7.6947e-01],\n",
      "        [ 5.8189e-03, -1.9906e-01],\n",
      "        [-9.3835e-02, -9.5281e-01],\n",
      "        [-3.0635e-01,  1.4042e-01],\n",
      "        [-4.2109e-01, -2.0556e-01],\n",
      "        [-1.7527e-01, -1.0641e-01],\n",
      "        [ 2.7669e-01, -1.8642e-01],\n",
      "        [-4.4987e-01, -7.9414e-01],\n",
      "        [ 1.1795e-01, -4.2774e-01],\n",
      "        [-8.9060e-02, -7.8607e-01],\n",
      "        [-4.1985e-02, -3.3760e-01],\n",
      "        [-9.8229e-02, -1.0432e-01],\n",
      "        [-3.5757e-01,  1.0358e-01],\n",
      "        [-1.8934e-01, -3.7880e-01],\n",
      "        [-1.4021e-01, -8.3698e-01],\n",
      "        [-1.0737e-01, -3.4037e-02],\n",
      "        [-4.0926e-01, -1.8990e-01],\n",
      "        [-4.7352e-01, -1.4083e-01],\n",
      "        [-4.5124e-01, -1.7893e-01],\n",
      "        [-2.9027e-01, -1.2965e-01],\n",
      "        [-3.9065e-01, -6.8669e-02],\n",
      "        [-3.9111e-01,  5.1493e-02],\n",
      "        [ 2.1861e-02, -6.0869e-01],\n",
      "        [-4.7250e-01, -3.9058e-03],\n",
      "        [-2.0637e-01, -5.5605e-01],\n",
      "        [-5.4208e-01, -2.1645e-02],\n",
      "        [-1.8311e-01, -6.5073e-01],\n",
      "        [ 6.8386e-03, -6.9014e-01],\n",
      "        [-1.7507e-01,  1.0466e-01],\n",
      "        [ 2.1789e-02, -3.0924e-01],\n",
      "        [-5.5335e-01, -5.8076e-01],\n",
      "        [-1.7411e-01,  1.0372e-01],\n",
      "        [-2.0269e-01,  1.9727e-01],\n",
      "        [ 2.3733e-01, -2.9645e-01],\n",
      "        [-3.2151e-01, -1.4139e-01],\n",
      "        [ 2.8654e-01, -4.5579e-01],\n",
      "        [ 2.1878e-02,  3.0694e-01],\n",
      "        [ 7.6157e-04, -6.0174e-01],\n",
      "        [ 1.9133e-01, -5.2058e-01],\n",
      "        [ 1.2740e-01, -3.8346e-01],\n",
      "        [ 4.7746e-02, -6.7720e-01],\n",
      "        [-9.6394e-02, -2.6452e-01],\n",
      "        [ 2.6698e-01, -5.9966e-01],\n",
      "        [-1.2314e-01, -8.2587e-01],\n",
      "        [-3.1578e-01, -6.4070e-01],\n",
      "        [-3.4341e-01, -3.1630e-01],\n",
      "        [-6.1222e-01, -5.2834e-01],\n",
      "        [-2.9090e-02, -5.0834e-01],\n",
      "        [-2.5368e-02,  3.8547e-02],\n",
      "        [ 1.1771e-02,  1.7856e-01],\n",
      "        [ 2.2090e-01,  2.0557e-02],\n",
      "        [ 6.1815e-02, -5.3690e-01],\n",
      "        [-1.2602e-01, -5.9583e-01],\n",
      "        [-4.0777e-01, -3.2514e-01],\n",
      "        [-3.3445e-01, -1.1367e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-2.2608e-01, -1.7130e-01],\n",
      "        [-2.1783e-01, -1.3661e-01],\n",
      "        [-1.8766e-01, -4.8199e-01],\n",
      "        [-1.1579e-01,  4.7605e-02],\n",
      "        [ 1.9484e-02, -5.2223e-01],\n",
      "        [-1.6642e-01,  1.2819e-03],\n",
      "        [ 2.2491e-01, -8.5011e-01],\n",
      "        [ 1.2288e-02,  3.8038e-01],\n",
      "        [-2.2524e-01,  5.8916e-02],\n",
      "        [-2.7987e-02, -6.9032e-01],\n",
      "        [-3.9042e-01, -4.1982e-01],\n",
      "        [-1.7011e-03, -2.2246e-01],\n",
      "        [-3.6188e-01, -2.7600e-01],\n",
      "        [-2.3645e-01, -2.2348e-01],\n",
      "        [-1.7323e-01, -1.6637e-01],\n",
      "        [-1.9487e-01, -9.0894e-03],\n",
      "        [-2.5124e-01, -1.9206e-01],\n",
      "        [-2.7245e-01, -4.5723e-01],\n",
      "        [ 3.9574e-01, -6.5072e-01],\n",
      "        [-2.2152e-01, -7.5564e-01],\n",
      "        [-1.3506e-01, -3.2909e-01],\n",
      "        [-5.6899e-02,  2.9397e-01],\n",
      "        [ 1.2606e-01, -3.2137e-01],\n",
      "        [-1.3494e-01,  5.0958e-02],\n",
      "        [-2.7323e-01, -2.0010e-01],\n",
      "        [-4.4665e-01, -2.4274e-01],\n",
      "        [-1.5306e-02, -6.7099e-01],\n",
      "        [-2.8745e-02, -6.8571e-01],\n",
      "        [-7.7998e-02, -3.6752e-01],\n",
      "        [ 2.8592e-01, -1.9904e-01],\n",
      "        [-1.9987e-01, -4.8402e-01],\n",
      "        [-7.3248e-02, -2.5786e-01],\n",
      "        [ 6.6285e-03, -4.7168e-04],\n",
      "        [ 2.7474e-02, -3.6880e-01],\n",
      "        [ 6.2393e-02, -4.6151e-01],\n",
      "        [-9.7925e-02,  2.7345e-01],\n",
      "        [ 4.1481e-02, -2.8527e-01],\n",
      "        [-1.8088e-01, -6.6299e-02],\n",
      "        [-4.2119e-01, -2.5848e-01],\n",
      "        [-2.8704e-01, -2.8238e-01],\n",
      "        [-1.0562e-01, -8.7119e-02],\n",
      "        [-4.4397e-01, -2.7438e-01],\n",
      "        [-1.7084e-01, -1.1591e-01],\n",
      "        [ 1.7867e-01, -2.0929e-01],\n",
      "        [-2.3867e-01,  1.0060e-02],\n",
      "        [-4.5402e-01, -3.2982e-01],\n",
      "        [-4.0916e-01, -7.3147e-01],\n",
      "        [-5.6878e-01, -1.5761e-01],\n",
      "        [-9.8888e-02,  1.4233e-01],\n",
      "        [-2.0004e-01,  2.2784e-01],\n",
      "        [-9.4963e-02,  3.5115e-02],\n",
      "        [-6.8692e-02,  1.6712e-02],\n",
      "        [-1.9061e-01, -4.6998e-01],\n",
      "        [-2.8905e-02, -1.8435e-01],\n",
      "        [-1.7329e-01, -1.3826e-01],\n",
      "        [-9.4393e-02, -1.8330e-01],\n",
      "        [-5.2632e-01, -3.3197e-01],\n",
      "        [-2.9706e-01, -1.7573e-01],\n",
      "        [-5.4559e-02, -2.0162e-01],\n",
      "        [-1.3598e-01,  3.9609e-02],\n",
      "        [-1.4024e-01, -3.8836e-01],\n",
      "        [-4.0282e-02,  2.7931e-01],\n",
      "        [ 2.7880e-01,  1.3541e-01],\n",
      "        [-3.6561e-01, -4.3462e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-0.0567, -0.5620],\n",
      "        [-0.2679, -0.1514],\n",
      "        [ 0.3559,  0.2190],\n",
      "        [ 0.1447,  0.4096],\n",
      "        [ 0.3241, -0.5841],\n",
      "        [ 0.0011, -0.4720],\n",
      "        [-0.0853, -0.2299],\n",
      "        [-0.3818, -0.5034],\n",
      "        [-0.2264, -0.0711],\n",
      "        [-0.2441, -0.5170],\n",
      "        [-0.1038, -0.2509],\n",
      "        [-0.4210,  0.4298],\n",
      "        [ 0.0580, -0.5397],\n",
      "        [-0.4496, -0.1066],\n",
      "        [-0.0099, -0.5779],\n",
      "        [ 0.0298, -0.8897],\n",
      "        [-0.3723,  0.0323],\n",
      "        [-0.1436, -0.0897],\n",
      "        [-0.0962, -0.2211],\n",
      "        [-0.0238, -0.1902],\n",
      "        [ 0.1397, -0.0727],\n",
      "        [ 0.2092, -0.5608],\n",
      "        [-0.4919, -0.3641],\n",
      "        [-0.2501,  0.2434],\n",
      "        [-0.3385,  0.1330],\n",
      "        [-0.4511,  0.2821],\n",
      "        [-0.3727, -0.3190],\n",
      "        [-0.3899,  0.0236],\n",
      "        [-0.2356, -0.2095],\n",
      "        [-0.1271, -0.2706],\n",
      "        [-0.0961,  0.1365],\n",
      "        [ 0.0175, -0.3236],\n",
      "        [ 0.3416, -0.2734],\n",
      "        [-0.2537, -0.5143],\n",
      "        [ 0.5487, -0.6501],\n",
      "        [-0.0340, -0.4429],\n",
      "        [-0.2160, -0.5058],\n",
      "        [-0.2935, -0.5178],\n",
      "        [-0.3419, -0.2255],\n",
      "        [-0.2706, -0.6351],\n",
      "        [-0.2021, -0.1740],\n",
      "        [ 0.2589, -0.3385],\n",
      "        [ 0.0644, -0.1004],\n",
      "        [-0.2190,  0.0538],\n",
      "        [-0.3042,  0.0248],\n",
      "        [-0.1850, -0.2517],\n",
      "        [-0.0937, -0.0086],\n",
      "        [-0.3214, -0.0627],\n",
      "        [-0.4269, -0.5557],\n",
      "        [-0.5522, -0.1223],\n",
      "        [ 0.0931,  0.0754],\n",
      "        [-0.0870, -0.2122],\n",
      "        [-0.1757, -0.1907],\n",
      "        [-0.2439, -0.4946],\n",
      "        [ 0.0292,  0.3928],\n",
      "        [ 0.0573,  0.1142],\n",
      "        [-0.3708, -0.6516],\n",
      "        [-0.1852, -0.3296],\n",
      "        [-0.2130, -0.3045],\n",
      "        [-0.2234,  0.1371],\n",
      "        [-0.1262, -0.4508],\n",
      "        [ 0.1205, -0.2320],\n",
      "        [ 0.0559,  0.4552],\n",
      "        [-0.2862, -0.0429]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-0.4018, -0.0694],\n",
      "        [ 0.0067,  0.3810],\n",
      "        [-0.0996, -0.6314],\n",
      "        [ 0.2337,  0.0284],\n",
      "        [-0.0131, -0.4057],\n",
      "        [ 0.0231, -0.4593],\n",
      "        [ 0.0383, -0.5036],\n",
      "        [ 0.2535, -0.7314],\n",
      "        [-0.3659, -0.1830],\n",
      "        [-0.2245, -0.4544],\n",
      "        [-0.4396, -0.0393],\n",
      "        [ 0.3026, -0.2995],\n",
      "        [-0.1389, -0.0400],\n",
      "        [-0.0789,  0.0497],\n",
      "        [-0.3104, -0.0025],\n",
      "        [-0.1725, -0.0731],\n",
      "        [-0.1636, -0.9136],\n",
      "        [ 0.0916, -0.3683],\n",
      "        [-0.3033,  0.0737],\n",
      "        [ 0.0632,  0.1942],\n",
      "        [ 0.2591, -0.7328],\n",
      "        [ 0.1073,  0.1272],\n",
      "        [-0.2687,  0.1849],\n",
      "        [-0.1859, -0.3996],\n",
      "        [ 0.0646, -0.2214],\n",
      "        [-0.2221, -0.4366],\n",
      "        [ 0.1800, -0.5145],\n",
      "        [-0.3626, -0.1012],\n",
      "        [-0.0873, -0.5155],\n",
      "        [ 0.3268, -0.0729],\n",
      "        [-0.0527,  0.0706],\n",
      "        [ 0.1217,  0.1672],\n",
      "        [-0.0586, -0.0963],\n",
      "        [-0.1553,  0.2021],\n",
      "        [ 0.1654, -0.0904],\n",
      "        [-0.0591, -0.5113],\n",
      "        [-0.1485, -0.2459],\n",
      "        [-0.2145,  0.0237],\n",
      "        [-0.3267, -0.2396],\n",
      "        [-0.2555, -0.1499],\n",
      "        [-0.2180, -0.5620],\n",
      "        [ 0.2010, -0.0061],\n",
      "        [-0.2188, -0.2311],\n",
      "        [-0.5415, -0.1354],\n",
      "        [-0.3081,  0.1143],\n",
      "        [-0.1958, -0.4221],\n",
      "        [ 0.2089, -0.4353],\n",
      "        [-0.3566, -0.2970],\n",
      "        [-0.3042,  0.3985],\n",
      "        [-0.3084,  0.2526],\n",
      "        [-0.1424,  0.1620],\n",
      "        [ 0.0396,  0.0915],\n",
      "        [-0.0612, -0.1612],\n",
      "        [-0.3512, -0.7632],\n",
      "        [-0.1346, -0.0604],\n",
      "        [ 0.1881, -0.2551],\n",
      "        [ 0.1250, -0.7005],\n",
      "        [-0.1322, -0.2732],\n",
      "        [-0.0548, -0.5196],\n",
      "        [-0.3586, -0.8906],\n",
      "        [ 0.4455, -0.4377],\n",
      "        [-0.3401, -0.5290],\n",
      "        [ 0.1404,  0.3008],\n",
      "        [ 0.0709, -0.4046]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-0.1842,  0.0484],\n",
      "        [-0.5352, -0.0736],\n",
      "        [-0.2591, -0.3718],\n",
      "        [-0.3204, -0.0429],\n",
      "        [-0.3536, -0.1564],\n",
      "        [-0.3544, -0.3389],\n",
      "        [-0.1317, -0.3347],\n",
      "        [-0.6355, -0.2329],\n",
      "        [-0.5080, -0.5009],\n",
      "        [-0.4175, -0.3478],\n",
      "        [-0.1277, -0.2638],\n",
      "        [-0.1432, -0.8303],\n",
      "        [-0.4734, -0.5044],\n",
      "        [ 0.5352, -0.1999],\n",
      "        [ 0.1114, -0.7001],\n",
      "        [ 0.7188,  0.1897],\n",
      "        [ 0.2608,  0.0829],\n",
      "        [-0.0547,  0.0921],\n",
      "        [ 0.1188, -0.1409],\n",
      "        [-0.0458,  0.4774],\n",
      "        [-0.1011,  0.3186],\n",
      "        [    nan,     nan],\n",
      "        [-0.3951, -0.2804],\n",
      "        [-0.4908, -0.7746],\n",
      "        [-0.4776, -0.0853],\n",
      "        [-0.3521,  0.1422],\n",
      "        [-0.7877, -0.1499],\n",
      "        [ 0.0133, -0.3486],\n",
      "        [ 0.1283,  0.3069],\n",
      "        [-0.2417,  0.0276],\n",
      "        [-0.0368, -0.4081],\n",
      "        [-0.1481, -0.5451],\n",
      "        [ 0.0422, -0.3646],\n",
      "        [-0.5394, -0.3366],\n",
      "        [-0.2147, -0.1662],\n",
      "        [-0.4015,  0.1785],\n",
      "        [-0.1809, -0.3200],\n",
      "        [ 0.3004, -0.1850],\n",
      "        [-0.4110, -0.5609],\n",
      "        [-0.1269, -0.4210],\n",
      "        [-0.5928, -0.4364],\n",
      "        [ 0.2229,  0.0692],\n",
      "        [-0.2753, -0.5716],\n",
      "        [-0.3416,  0.0297],\n",
      "        [-0.0994, -0.0243],\n",
      "        [ 0.0548, -0.6855],\n",
      "        [-0.0125, -0.0064],\n",
      "        [-0.2345, -0.3554],\n",
      "        [-0.0368, -0.6030],\n",
      "        [-0.3169, -0.0638],\n",
      "        [-0.2533, -0.3924],\n",
      "        [-0.0783, -0.0614],\n",
      "        [ 0.3113, -0.2544],\n",
      "        [-0.2967,  0.0469],\n",
      "        [-0.2731,  0.1210],\n",
      "        [ 0.2402, -0.2448],\n",
      "        [-0.0068, -0.8242],\n",
      "        [-0.2324, -0.5934],\n",
      "        [-0.1954, -0.5143],\n",
      "        [-0.0238, -0.2401],\n",
      "        [ 0.0150, -0.7324],\n",
      "        [-0.2830, -0.1349],\n",
      "        [-0.4324,  0.2389],\n",
      "        [ 0.0188, -0.2687]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "DINOLinearClassifier(\n",
      "  (transformer): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "Training Loss: 0.0000, Accuracy: 0.5799\n"
     ]
    }
   ],
   "source": [
    "total_loss, total_accuracy = 0, 0\n",
    "preds_list = [] # store predictions\n",
    "label_ids_list = [] # store labels\n",
    "for batch in dataloaders[\"train\"]:\n",
    "    inputs, labels = batch[0].to(train_config.device), batch[1].to(train_config.device)\n",
    "    optimizer.zero_grad()\n",
    "    print(model)\n",
    "    outputs = model(inputs.float())\n",
    "    print(outputs)\n",
    "    print(type(outputs))\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    preds = outputs.argmax(dim=1)\n",
    "    preds_list.extend(preds.cpu().numpy())\n",
    "    label_ids_list.extend(labels.cpu().numpy())\n",
    "    total_accuracy += (preds == labels).float().mean().item()\n",
    "avg_loss = total_loss / len(dataloaders[\"train\"])\n",
    "avg_accuracy = total_accuracy / len(dataloaders[\"train\"])\n",
    "print(f\"Training Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4193548262119293"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds == labels).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('sarl_env_xformers': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48ee45326076df0fbb8b08362e1575ed4c680b2045926997f19ad5e148cfebef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
